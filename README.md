# LLM

[English](README_en.md)

# ğŸŒŸ å¤§æ¨¡å‹å­¦ä¹ å†ç¨‹

> ğŸ‘¤ ä½œè€…: [@shangguanjiannan](https://github.com/shangguanjiannan)  
> ğŸ“… èµ·å§‹æ—¶é—´: 2025  
> ğŸ› ï¸ æœ€è¿‘æ›´æ–°: 2025-07-31  
> ğŸ§· å…³é”®è¯: LLM, Transformer, å¾®è°ƒ, æ£€ç´¢å¢å¼ºç”Ÿæˆ, æ¨ç†éƒ¨ç½², Tokenizer  

## ğŸ§  å­¦ä¹ èƒŒæ™¯

éšç€å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„çˆ†å‘ï¼Œç†è§£å…¶åŸç†å¹¶æŒæ¡è½åœ°æ–¹æ³•æˆä¸º AI å·¥ç¨‹å¸ˆçš„æ ¸å¿ƒèƒ½åŠ›ã€‚è®°å½•äº†æˆ‘åœ¨ LLM å­¦ä¹ ä¸å®æˆ˜è¿‡ç¨‹ä¸­çš„æ€è€ƒä¸æŠ€æœ¯ç§¯ç´¯ã€‚

## ğŸ—‚ï¸ å­¦ä¹ ç›®å½•

1. åŸºç¡€ç†è®º  
2. æ¨¡å‹è®­ç»ƒ  
3. æ¨ç†éƒ¨ç½²  
4. RAG æ£€ç´¢å¢å¼ºç”Ÿæˆ  
5. å®æˆ˜é¡¹ç›®  
6. è¸©å‘ç¬”è®°  
7. å‚è€ƒèµ„æº  

## ğŸ“˜ åŸºç¡€ç†è®º

### ğŸ”¸ Transformer æ¶æ„

- åŸå§‹è®ºæ–‡: *Attention Is All You Need*  
- æ ¸å¿ƒæ¨¡å—: å¤šå¤´æ³¨æ„åŠ›ã€å‰é¦ˆç½‘ç»œã€æ®‹å·®è¿æ¥+å½’ä¸€åŒ–  
- æ¨¡å‹ç±»å‹:  
  - Decoder-onlyï¼ˆå¦‚ GPTï¼‰  
  - Encoder-onlyï¼ˆå¦‚ BERTï¼‰  
  - Encoder-Decoderï¼ˆå¦‚ T5/BARTï¼‰

### ğŸ”¸ åˆ†è¯æœºåˆ¶

- å¸¸ç”¨æ–¹æ³•: BPEã€Unigramã€SentencePiece  
- å·¥å…·æ¨è: Huggingface Tokenizersã€SentencePieceã€Tiktoken  
- ä¸­æ–‡æ”¯æŒ: `bge`, `text2vec`, `QwenTokenizer`

## ğŸ§ª æ¨¡å‹è®­ç»ƒ

### ğŸ”¹ é¢„è®­ç»ƒ

- æ•°æ®å¤„ç†ï¼šå»é‡ã€æ¸…æ´—ã€æ ¼å¼æ ‡å‡†åŒ–  
- ä½¿ç”¨å·¥å…·ï¼š`transformers`ã€`OLMo`ã€`DeepSpeed`ã€`Axolotl`  
- é…ç½®ç®¡ç†ï¼š`config.json`, `tokenizer_config.json`

### ğŸ”¹ æŒ‡ä»¤å¾®è°ƒ SFT

```json
{"instruction": "ä½ æ˜¯è°", "input": "", "output": "æˆ‘æ˜¯AIåŠ©æ‰‹"}
```

- æ ¼å¼æ”¯æŒï¼šOpenAIã€Alpacaã€ChatML  
- æ¨èå†™æ³•ï¼šå¤šæ¨¡æ¿æ”¯æŒ + æ‰¹é‡ç”Ÿæˆè„šæœ¬  

## âš™ï¸ æ¨ç†éƒ¨ç½²

### ğŸ”¹ ä½¿ç”¨ vLLM éƒ¨ç½²

```bash
CUDA_VISIBLE_DEVICES=6,7 python -m vllm.entrypoints.openai.api_server   --served-model-name qwen3-14b   --model /path/to/qwen3-14b   --tensor-parallel-size 2   --max-model-len 32000   --port 8051
```

- æ˜¾å­˜ä¼˜åŒ–ï¼š  
  ```bash
  export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
  export VLLM_DISABLE_TORCH_COMPILE=1
  export TORCHDYNAMO_DISABLE=1
  ```

- Torch >= 2.3 æ—¶éœ€å…³é—­ torch.compileï¼Œé˜²æ­¢ kernel æŠ¥é”™  

### ğŸ”¹ æ¨¡å‹æ ¼å¼è½¬æ¢ï¼ˆOLMo â†’ HFï¼‰

- ç¤ºä¾‹è·¯å¾„: `/mnt/.../Pretrain/CNIT-1B-TDDRSIMPLE`  
- è½¬æ¢è¾“å‡ºï¼šåˆ†ç‰‡ `.safetensors` + `config.json` + è‡ªå®šä¹‰ `tokenizer.json`

### ğŸ”¹ Tokenizer å†²çªä¿®å¤

- æŠ¥é”™ä½ç½®ï¼š`AutoConfig.register("aimv2", AIMv2Config)`  
- ä¿®å¤æ–¹å¼ï¼šæ³¨é‡Šè¯¥è¡Œä»£ç 

## ğŸ” RAG æ£€ç´¢å¢å¼ºç”Ÿæˆ

- ç‰¹å¾æ¨¡å‹æ¨èï¼š`all-MiniLM-L6-v2`, `bge`, `text2vec`  
- Milvus æ£€ç´¢ç³»ç»Ÿä½¿ç”¨æ­¥éª¤ï¼šschema â†’ æ’å…¥ â†’ ç´¢å¼• â†’ æŸ¥è¯¢  

```python
from pymilvus import Collection
collection = Collection("mcp-errors")
collection.search(...)
```

## ğŸ› ï¸ å®æˆ˜é¡¹ç›®

### MCP-RAG

- æ ¸å¿ƒç›®æ ‡ï¼šæ„å»ºé”™è¯¯æ—¥å¿—çŸ¥è¯†åº“ + è‡ªåŠ¨é—®ç­”  
- å…³é”®ç»„ä»¶ï¼šå‘é‡ç´¢å¼•ã€è¯­ä¹‰æ£€ç´¢ã€æŒ‡ä»¤å¾®è°ƒå›å¤

## ğŸ› è¸©å‘ç¬”è®°

| é—®é¢˜ | è§£å†³æ–¹æ³• |
|------|----------|
| np.float è¢«åºŸå¼ƒ | æ”¹ä¸º np.float64 |
| PY_SSIZE_T_CLEAN æŠ¥é”™ | å¢åŠ å®å®šä¹‰é‡æ–°ç¼–è¯‘ |
| AIMv2 æ³¨å†Œå†²çª | æ³¨é‡Šæ‰æ³¨å†Œé¡¹ |
| torch.compile æŠ¥é”™ | å…³é—­ vLLM ä¸­çš„ compile |

## ğŸ“š å‚è€ƒèµ„æº

- [Transformers æ–‡æ¡£](https://huggingface.co/docs/transformers)
- [OLMo é¡¹ç›®](https://github.com/allenai/OLMo)
- [vLLM é¡¹ç›®](https://github.com/vllm-project/vllm)
- [Milvus æ•°æ®åº“](https://milvus.io/)
- [Qwen é¡¹ç›®](https://github.com/QwenLM)