
<!-- ä¸­è‹±åˆ‡æ¢æŒ‰é’®æ ·å¼ä¸è„šæœ¬ -->
<style>
.toggle-button {
  margin: 1rem 0;
  padding: 6px 12px;
  background-color: #3b82f6;
  color: white;
  border: none;
  border-radius: 6px;
  cursor: pointer;
}
.lang-cn { display: block; }
.lang-en { display: none; }
</style>

<button class="toggle-button" onclick="toggleLang()">ğŸŒ åˆ‡æ¢è¯­è¨€ / Switch Language</button>

<script>
let showCN = true;
function toggleLang() {
  showCN = !showCN;
  document.querySelectorAll('.lang-cn').forEach(el => el.style.display = showCN ? 'block' : 'none');
  document.querySelectorAll('.lang-en').forEach(el => el.style.display = showCN ? 'none' : 'block');
}
</script>

---

<div class="lang-cn">

# ğŸŒŸ å¤§æ¨¡å‹å­¦ä¹ å†ç¨‹

> ğŸ‘¤ ä½œè€…: [@shangguanjiannan](#)  
> ğŸ“… èµ·å§‹æ—¶é—´: 2025  
> ğŸ› ï¸ æœ€è¿‘æ›´æ–°: 2025-07-31  
> ğŸ§· å…³é”®è¯: LLM, Transformer, å¾®è°ƒ, æ£€ç´¢å¢å¼ºç”Ÿæˆ, æ¨ç†éƒ¨ç½², Tokenizer  

## ğŸ§  å­¦ä¹ èƒŒæ™¯

éšç€å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„çˆ†å‘ï¼Œç†è§£å…¶åŸç†å¹¶æŒæ¡è½åœ°æ–¹æ³•æˆä¸º AI å·¥ç¨‹å¸ˆçš„æ ¸å¿ƒèƒ½åŠ›ã€‚è®°å½•äº†æˆ‘åœ¨ LLM å­¦ä¹ ä¸å®æˆ˜è¿‡ç¨‹ä¸­çš„æ€è€ƒä¸æŠ€æœ¯ç§¯ç´¯ã€‚

## ğŸ—‚ï¸ å­¦ä¹ ç›®å½•

1. åŸºç¡€ç†è®º  
2. æ¨¡å‹è®­ç»ƒ  
3. æ¨ç†éƒ¨ç½²  
4. RAG æ£€ç´¢å¢å¼ºç”Ÿæˆ  
5. å®æˆ˜é¡¹ç›®  
6. è¸©å‘ç¬”è®°  
7. å‚è€ƒèµ„æº  

## ğŸ“˜ åŸºç¡€ç†è®º

### ğŸ”¸ Transformer æ¶æ„

- åŸå§‹è®ºæ–‡: *Attention Is All You Need*  
- æ ¸å¿ƒæ¨¡å—: å¤šå¤´æ³¨æ„åŠ›ã€å‰é¦ˆç½‘ç»œã€æ®‹å·®è¿æ¥+å½’ä¸€åŒ–  
- æ¨¡å‹ç±»å‹:  
  - Decoder-onlyï¼ˆå¦‚ GPTï¼‰  
  - Encoder-onlyï¼ˆå¦‚ BERTï¼‰  
  - Encoder-Decoderï¼ˆå¦‚ T5/BARTï¼‰

### ğŸ”¸ åˆ†è¯æœºåˆ¶

- å¸¸ç”¨æ–¹æ³•: BPEã€Unigramã€SentencePiece  
- å·¥å…·æ¨è: Huggingface Tokenizersã€SentencePieceã€Tiktoken  
- ä¸­æ–‡æ”¯æŒ: `bge`, `text2vec`, `QwenTokenizer`

## ğŸ§ª æ¨¡å‹è®­ç»ƒ

### ğŸ”¹ é¢„è®­ç»ƒ

- æ•°æ®å¤„ç†ï¼šå»é‡ã€æ¸…æ´—ã€æ ¼å¼æ ‡å‡†åŒ–  
- ä½¿ç”¨å·¥å…·ï¼š`transformers`ã€`OLMo`ã€`DeepSpeed`ã€`Axolotl`  
- é…ç½®ç®¡ç†ï¼š`config.json`, `tokenizer_config.json`

### ğŸ”¹ æŒ‡ä»¤å¾®è°ƒ SFT

```json
{"instruction": "ä½ æ˜¯è°", "input": "", "output": "æˆ‘æ˜¯AIåŠ©æ‰‹"}
```

- æ ¼å¼æ”¯æŒï¼šOpenAIã€Alpacaã€ChatML  
- æ¨èå†™æ³•ï¼šå¤šæ¨¡æ¿æ”¯æŒ + æ‰¹é‡ç”Ÿæˆè„šæœ¬  

## âš™ï¸ æ¨ç†éƒ¨ç½²

### ğŸ”¹ ä½¿ç”¨ vLLM éƒ¨ç½²

```bash
CUDA_VISIBLE_DEVICES=6,7 python -m vllm.entrypoints.openai.api_server   --served-model-name qwen3-14b   --model /path/to/qwen3-14b   --tensor-parallel-size 2   --max-model-len 32000   --port 8051
```

- æ˜¾å­˜ä¼˜åŒ–ï¼š  
  ```bash
  export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
  export VLLM_DISABLE_TORCH_COMPILE=1
  export TORCHDYNAMO_DISABLE=1
  ```

- Torch >= 2.3 æ—¶éœ€å…³é—­ torch.compileï¼Œé˜²æ­¢ kernel æŠ¥é”™  

### ğŸ”¹ æ¨¡å‹æ ¼å¼è½¬æ¢ï¼ˆOLMo â†’ HFï¼‰

- ç¤ºä¾‹è·¯å¾„: `/mnt/.../Pretrain/CNIT-1B-TDDRSIMPLE`  
- è½¬æ¢è¾“å‡ºï¼šåˆ†ç‰‡ `.safetensors` + `config.json` + è‡ªå®šä¹‰ `tokenizer.json`

### ğŸ”¹ Tokenizer å†²çªä¿®å¤

- æŠ¥é”™ä½ç½®ï¼š`AutoConfig.register("aimv2", AIMv2Config)`  
- ä¿®å¤æ–¹å¼ï¼šæ³¨é‡Šè¯¥è¡Œä»£ç 

## ğŸ” RAG æ£€ç´¢å¢å¼ºç”Ÿæˆ

- ç‰¹å¾æ¨¡å‹æ¨èï¼š`all-MiniLM-L6-v2`, `bge`, `text2vec`  
- Milvus æ£€ç´¢ç³»ç»Ÿä½¿ç”¨æ­¥éª¤ï¼šschema â†’ æ’å…¥ â†’ ç´¢å¼• â†’ æŸ¥è¯¢  

```python
from pymilvus import Collection
collection = Collection("mcp-errors")
collection.search(...)
```

## ğŸ› ï¸ å®æˆ˜é¡¹ç›®

### MCP-RAG

- æ ¸å¿ƒç›®æ ‡ï¼šæ„å»ºé”™è¯¯æ—¥å¿—çŸ¥è¯†åº“ + è‡ªåŠ¨é—®ç­”  
- å…³é”®ç»„ä»¶ï¼šå‘é‡ç´¢å¼•ã€è¯­ä¹‰æ£€ç´¢ã€æŒ‡ä»¤å¾®è°ƒå›å¤

## ğŸ› è¸©å‘ç¬”è®°

| é—®é¢˜ | è§£å†³æ–¹æ³• |
|------|----------|
| np.float è¢«åºŸå¼ƒ | æ”¹ä¸º np.float64 |
| PY_SSIZE_T_CLEAN æŠ¥é”™ | å¢åŠ å®å®šä¹‰é‡æ–°ç¼–è¯‘ |
| AIMv2 æ³¨å†Œå†²çª | æ³¨é‡Šæ‰æ³¨å†Œé¡¹ |
| torch.compile æŠ¥é”™ | å…³é—­ vLLM ä¸­çš„ compile |

## ğŸ“š å‚è€ƒèµ„æº

- [Transformers æ–‡æ¡£](https://huggingface.co/docs/transformers)
- [OLMo é¡¹ç›®](https://github.com/allenai/OLMo)
- [vLLM é¡¹ç›®](https://github.com/vllm-project/vllm)
- [Milvus æ•°æ®åº“](https://milvus.io/)
- [Qwen é¡¹ç›®](https://github.com/QwenLM)

</div>

<div class="lang-en">

# ğŸŒŸ LLM Study Journey

> ğŸ‘¤ Author: [@shangguanjiannan](#)  
> ğŸ“… Started: 2025  
> ğŸ› ï¸ Last Updated: 2025-07-31  
> ğŸ§· Keywords: LLM, Transformer, Fine-tuning, RAG, Inference, Tokenizer

## ğŸ§  Background

With the boom of Large Language Models (LLMs), understanding their mechanisms and implementation has become essential. This document records my insights, hands-on projects, and troubleshooting during the learning journey.

## ğŸ—‚ï¸ Table of Contents

1. Fundamentals  
2. Training  
3. Inference & Deployment  
4. RAG (Retrieval-Augmented Generation)  
5. Projects  
6. Troubleshooting Notes  
7. References  

## ğŸ“˜ Fundamentals

### ğŸ”¸ Transformer Architecture

- Original paper: *Attention Is All You Need*  
- Key modules: Multi-head attention, Feedforward layers, Residual + LayerNorm  
- Model types:  
  - Decoder-only (e.g., GPT)  
  - Encoder-only (e.g., BERT)  
  - Encoder-Decoder (e.g., T5/BART)

### ğŸ”¸ Tokenization

- Methods: BPE, Unigram, WordPiece, SentencePiece  
- Tools: Huggingface Tokenizers, SentencePiece, Tiktoken  
- For Chinese: `bge`, `text2vec`, `QwenTokenizer`

## ğŸ§ª Training

### ğŸ”¹ Pretraining

- Preprocessing: deduplication, cleaning, formatting  
- Tools: `transformers`, `OLMo`, `DeepSpeed`, `Axolotl`  
- Config: `config.json`, `tokenizer_config.json`

### ğŸ”¹ Supervised Fine-tuning (SFT)

```json
{"instruction": "Who are you?", "input": "", "output": "I'm an AI assistant."}
```

- Format styles: Alpaca, ChatML, OpenAI  
- Tips: Use multiple templates and scripting to generate training data

## âš™ï¸ Inference & Deployment

### ğŸ”¹ Serving with vLLM

```bash
CUDA_VISIBLE_DEVICES=6,7 python -m vllm.entrypoints.openai.api_server   --served-model-name qwen3-14b   --model /path/to/qwen3-14b   --tensor-parallel-size 2   --max-model-len 32000   --port 8051
```

- Memory tuning:
  ```bash
  export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
  export VLLM_DISABLE_TORCH_COMPILE=1
  export TORCHDYNAMO_DISABLE=1
  ```

### ğŸ”¹ Convert OLMo to HF Format

- Path: `/mnt/.../Pretrain/CNIT-1B-TDDRSIMPLE`  
- Output: sharded `.safetensors`, `config.json`, and custom `tokenizer.json`

### ğŸ”¹ Tokenizer Conflict Fix

- Problem: `AutoConfig.register("aimv2", AIMv2Config)` causes duplicate entry  
- Fix: comment out the conflicting line

## ğŸ” RAG System

- Embedding Models: `all-MiniLM-L6-v2`, `bge`, `text2vec`  
- Milvus Workflow: schema â†’ insert â†’ index â†’ search  

```python
from pymilvus import Collection
collection = Collection("mcp-errors")
collection.search(...)
```

## ğŸ› ï¸ Projects

### MCP-RAG

- Goal: AI assistant for error log retrieval and reasoning  
- Stack: Milvus + SentenceTransformer + FastAPI + Instruction Tuning

## ğŸ› Troubleshooting Notes

| Issue | Solution |
|-------|----------|
| np.float deprecated | Use np.float64 |
| PY_SSIZE_T_CLEAN error | Add macro and recompile |
| AIMv2 register conflict | Comment out register line |
| torch.compile crash | Disable torch.compile for vLLM |

## ğŸ“š References

- [Transformers Docs](https://huggingface.co/docs/transformers)  
- [OLMo](https://github.com/allenai/OLMo)  
- [vLLM](https://github.com/vllm-project/vllm)  
- [Milvus](https://milvus.io/)  
- [Qwen](https://github.com/QwenLM)

</div>
